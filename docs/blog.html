<!DOCTYPE html>
<html lang="en">
</head>

<body class="selection:bg-cyan-500/30">
    
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

</head>


</head>
<head>
    <meta charset="UTF-8">
    <title>Blog & Research - Hektor</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:ital,wght@0,200;0,800;1,800&family=JetBrains+Mono:wght@300;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --haze-1: #1a0b2e;
            --haze-2: #091220;
            --ion-glow: #a78bfa;
            --ion-cyan: #22d3ee;
            --base-bg: #050505;
        }

        body {
            background-color: var(--base-bg);
            color: #fff;
            font-family: 'Plus Jakarta Sans', sans-serif;
            overflow-x: hidden;
            cursor: crosshair;
        }

        .mono { font-family: 'JetBrains Mono', monospace; }

        /* Kinetic Ionized Haze Effect */
        .haze-canvas {
            position: fixed;
            inset: 0;
            z-index: -1;
            background: radial-gradient(circle at var(--mx, 50%) var(--my, 50%), 
                        rgba(167, 139, 250, 0.15) 0%, 
                        rgba(34, 211, 238, 0.05) 30%, 
                        transparent 70%);
            filter: url(#ionize);
        }

        /* SVG Grain Overlay */
        .grain-layer {
            position: fixed;
            inset: 0;
            z-index: 100;
            pointer-events: none;
            opacity: 0.4;
            mix-blend-mode: overlay;
        }

        /* Ionized Text */
        .text-ion {
            background: linear-gradient(135deg, #fff 0%, var(--ion-glow) 50%, var(--ion-cyan) 100%);
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            position: relative;
        }

        .text-ion::after {
            content: attr(data-text);
            position: absolute;
            left: 0; top: 0;
            z-index: -1;
            filter: blur(15px);
            opacity: 0.6;
            background: inherit;
            -webkit-background-clip: text;
            background-clip: text;
        }

        /* Kinetic Shells (Non-generic cards) */
        .kinetic-shell {
            position: relative;
            background: rgba(255, 255, 255, 0.02);
            border-left: 1px solid rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            transition: all 0.6s cubic-bezier(0.16, 1, 0.3, 1);
            clip-path: polygon(0 0, 100% 0, 100% 85%, 90% 100%, 0 100%);
        }

        .kinetic-shell:hover {
            background: rgba(255, 255, 255, 0.05);
            border-left: 1px solid var(--ion-cyan);
            transform: translateX(10px);
        }

        .kinetic-shell::before {
            content: '';
            position: absolute;
            top: 0; left: 0; width: 100%; height: 2px;
            background: linear-gradient(90deg, var(--ion-cyan), transparent);
            transform: scaleX(0);
            transform-origin: left;
            transition: transform 0.6s ease;
        }

        .kinetic-shell:hover::before {
            transform: scaleX(1);
        }

        /* Ion Beam Button */
        .ion-btn {
            position: relative;
            padding: 1.25rem 3rem;
            font-weight: 800;
            letter-spacing: 0.2em;
            text-transform: uppercase;
            color: white;
            transition: 0.4s;
            overflow: hidden;
            border: 1px solid rgba(255,255,255,0.1);
        }

        .ion-btn:hover {
            background: white;
            color: black;
            box-shadow: 0 0 50px rgba(34, 211, 238, 0.4);
        }

        .ion-btn .beam {
            position: absolute;
            top: 0; left: -100%;
            width: 100%; height: 100%;
            background: linear-gradient(90deg, transparent, rgba(255,255,255,0.8), transparent);
            transition: 0.6s;
        }

        .ion-btn:hover .beam {
            left: 100%;
        }

        /* Data Stream Scroller */
        .data-stream {
            mask-image: linear-gradient(to bottom, transparent, black 20%, black 80%, transparent);
        }

        @keyframes pulse-ion {
            0%, 100% { opacity: 0.5; filter: blur(2px); }
            50% { opacity: 1; filter: blur(0px); }
        }

        .ion-indicator {
            width: 8px; height: 8px;
            background: var(--ion-cyan);
            border-radius: 50%;
            box-shadow: 0 0 15px var(--ion-cyan);
            animation: pulse-ion 2s infinite;
        }

        .reveal {
            opacity: 0;
            transform: translateY(40px) skewY(2deg);
            transition: all 1.2s cubic-bezier(0.16, 1, 0.3, 1);
        }

        .reveal.active {
            opacity: 1;
            transform: translateY(0) skewY(0deg);
        }

    </style>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</head>
    </div>

    <!-- Navigation -->
    <nav class="fixed top-0 w-full z-[200] px-10 py-8 flex justify-between items-center mix-blend-difference">
        <div class="flex items-center gap-6">
            <div class="flex flex-col">
                <span class="text-2xl font-black tracking-tighter">HEKTOR</span>
                <span class="mono text-[8px] tracking-[0.5em] text-zinc-500 uppercase">Kinetic Engine v4.0</span>
            </div>
        </div>
        <div class="hidden lg:flex gap-16 items-center">
            <div class="flex items-center gap-3">
                <div class="ion-indicator"></div>
                <span class="mono text-[10px] tracking-widest uppercase opacity-60">System: Stable</span>
            </div>
            <a href="#" class="mono text-[10px] tracking-[0.3em] uppercase hover:text-cyan-400 transition-colors">Documentation</a>
            <a href="#" class="mono text-[10px] tracking-[0.3em] uppercase hover:text-cyan-400 transition-colors">Vault</a>
            <button class="px-5 py-2 border border-white/20 mono text-[10px] uppercase hover:bg-white hover:text-black transition-all">Node: 01-A</button>
        </div>
    </nav>

    <div class="container">
        <!-- Hero -->
        <div class="hero">
            <div class="hero-eyebrow">Research & Insights</div>
            <h1>Building <strong>Hektor</strong></h1>
            <p>Notes from the frontier of vector databases, perceptual quantization, and high-dimensional search.</p>
        </div>

        <!-- Section Navigation -->
        <div class="section-nav">
            <button class="active" data-section="blog">Blog</button>
            <button data-section="articles">Articles</button>
            <button data-section="research">Research</button>
            <button data-section="publications">Publications</button>
        </div>

        <!-- Blog Section -->
        <div class="content-section active" id="blog-section">
            <div class="section-header">
                <h2>Developer Blog</h2>
                <p>Personal notes, decisions, and lessons from building Hektor</p>
            </div>
            <div class="card-grid" id="blog-grid"></div>
        </div>

        <!-- Articles Section -->
        <div class="content-section" id="articles-section">
            <div class="section-header">
                <h2>Technical Articles</h2>
                <p>Deep explorations into advanced topics and emerging research</p>
            </div>
            <div class="card-grid" id="articles-grid"></div>
        </div>

        <!-- Research Section -->
        <div class="content-section" id="research-section">
            <div class="section-header">
                <h2>Research Papers</h2>
                <p>Foundational research on algorithms, theory, and implementation</p>
            </div>
            <div class="card-grid" id="research-grid"></div>
        </div>

        <!-- Publications Section -->
        <div class="content-section" id="publications-section">
            <div class="section-header">
                <h2>Publications</h2>
                <p>Surveys, standards, and comprehensive analyses</p>
            </div>
            <div class="card-grid" id="publications-grid"></div>
        </div>
    </div>

    <!-- Modal -->
    <div class="modal-overlay" id="modal-overlay">
        <div class="modal">
            <div class="modal-header">
                <div>
                    <h2 id="modal-title"></h2>
                    <div class="meta" id="modal-meta"></div>
                </div>
                <button class="modal-close" id="modal-close">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M18 6L6 18M6 6l12 12"/>
                    </svg>
                </button>
            </div>
            <div class="modal-content">
                <div class="article-content" id="modal-content"></div>
            </div>
        </div>
    </div>

    <!-- Preloaded markdown (embedded so file:// previews work) -->
    <script type="text/markdown" id="md-blog_posts-the-beginning"># The Beginning: Why I Started Building Hektor

*November 15, 2025*

---

Every meaningful project starts with frustration. Mine began on a cold November night, staring at benchmark results that made no sense.

I'd been evaluating vector databases for a project. Pinecone. Milvus. Qdrant. FAISS. Each one excellent in its own way. Each one missing something I needed.

## The Problem

What I wanted was simple:

1. **Sub-millisecond search** at scale (millions of vectors)
2. **High recall** (99%+ wasn't negotiable)
3. **Hybrid search** — vectors + full-text, unified, _enterprised_.
4. **Production-ready** — not a research prototype

No single solution hit all four. FAISS was fast but didn't do hybrid search. Milvus had features but latency was unpredictable. Qdrant was elegant but I needed more control over quantization.

## The Decision

So I did what engineers do when tools don't exist: I decided to build one.

The name came easily. **Hektor** — the defender. In the Iliad, Hektor was the Trojan hero who stood against impossible odds. My Hektor would defend against the chaos of unstructured data, the flood of embeddings that modern AI systems generate.

## What I'm Building

The architecture crystallized over a few whiteboard sessions:

- **C++23 core** — no compromises on performance
- **HNSW index** — the best algorithm for approximate nearest neighbor
- **Perceptual quantization** — compress intelligently, not blindly
- **Hybrid search engine** — BM25 + vectors, fused properly
- **Python bindings** — because the world runs on Python

I'm not trying to replace everything. I'm building something that does fewer things, but does them exceptionally well.

## What's Next

This blog will document the journey. The decisions that worked, the mistakes I made, the algorithms I implemented wrong three times before getting them right.

Building in public is terrifying. But it's also the only way to build something that matters.

Let's see where this goes.

---

*— Hektor*</script>

    <script type="text/markdown" id="md-blog_posts-choosing-hnsw"># Why HNSW Won: Choosing Our Index Algorithm

*December 3, 2025*

---

Choosing an index algorithm is like choosing a foundation for a house. Get it wrong, and everything built on top will be compromised.

We spent three weeks evaluating options. Here's what we learned.

## The Contenders

### KD-Trees
The classic. Beautiful in low dimensions (≤20). Falls apart spectacularly above that. We're dealing with 384-512 dimensional vectors. Next.

### LSH (Locality-Sensitive Hashing)
Probabilistic guarantees sound good in papers. In practice, you need multiple hash tables to get decent recall, which eats memory. The parameter tuning is also dark magic — `k` hash functions, `L` tables, and pray.

### IVF (Inverted File Index)
FAISS made this popular. You cluster your data, then search relevant clusters. Fast builds. Good for batch workloads. But for real-time search with frequent updates? The cluster assignments become stale. Rebalancing is expensive.

### HNSW
A graph where every node connects to its approximate nearest neighbors, organized in hierarchical layers. Like a skip list, but for metric spaces.

## Why HNSW Won

The numbers spoke for themselves:

| Algorithm | Recall@10 | QPS (1M vectors) | Memory Overhead |
|-----------|-----------|------------------|-----------------|
| IVF-PQ | 0.89 | 12,000 | Low |
| LSH | 0.85 | 8,000 | High |
| HNSW | 0.98 | 5,500 | Medium |

HNSW had the highest recall. Not the fastest raw QPS, but fast enough — and when you factor in that IVF needs reranking to match HNSW's recall, the gap closes.

But the real winner was **operational simplicity**.

## The Operational Argument

HNSW has two main parameters:
- `M`: connections per node (we use 16)
- `ef_search`: search queue size (we use 200)

That's it. No cluster count to tune. No hash function parameters. No periodic rebalancing.

When a new vector comes in, we insert it. Takes ~2ms. The graph stays balanced automatically because of how the layers are structured.

For a production system, this simplicity is gold.

## Our Implementation

We wrote HNSW from scratch in C++23. Using hnswlib was tempting, but we needed:

1. **SIMD distance functions** — AVX-512 makes a 4x difference
2. **Custom memory layout** — cache-friendly node storage
3. **Concurrent reads during writes** — no global locks

The implementation took six weeks. Worth every hour.

## What We'd Do Differently

If we were building for 10B+ vectors, we'd probably use a hybrid approach — IVF for coarse partitioning, HNSW within partitions. But for our target scale (millions to low billions), pure HNSW is the right call.

---

*Next week: The perceptual quantization rabbit hole. It goes deep.*</script>

    <script type="text/markdown" id="md-blog_posts-quantization-rabbit-hole"># The Quantization Rabbit Hole

*December 18, 2025*

---

"We just need to compress the vectors a bit."

Famous last words.

What started as a simple optimization became a three-week journey through perceptual science, color theory, and the mathematics of human vision. Here's what we learned.

## The Naive Approach

Standard vector quantization is straightforward:

1. Take your 512-dimensional float32 vectors (2KB each)
2. Quantize to int8 using scalar quantization (512 bytes each)
3. Save 75% memory

This works. Recall drops from 99.7% to ~95%. Acceptable for some use cases.

But we wanted better.

## Enter Product Quantization

The key insight: divide your vector into subvectors, quantize each independently.

```
512-dim vector → 8 subvectors of 64 dims each
Each subvector → mapped to one of 256 centroids
Result: 8 bytes instead of 2048 bytes (256x compression!)
```

The magic is in training the centroids. You cluster your subvectors using k-means, and the centroids capture the structure of your data.

We implemented this. Recall: 97%. Memory: 8 bytes per vector.

But then someone asked: *"What if perception matters?"*

## The Perceptual Revelation

Here's a fact that broke my brain: human perception of intensity is **logarithmic**, not linear.

The difference between 0 and 1 (luminance) looks the same as 10 and 11 to a meter. But to your eyes, 0→1 is a massive change while 10→11 is barely noticeable.

This is the Weber-Fechner law. And it means: **uniform quantization wastes bits**.

If you allocate equal precision across the range, you're over-representing changes humans can't perceive and under-representing changes they can.

## PQ Curves and HDR

The video industry solved this decades ago with Perceptual Quantizer (PQ) curves — SMPTE ST.2084.

The curve maps linear luminance to a perceptually uniform space.

```
PQ(Y) = ((c₁ + c₂·Y^m₁) / (1 + c₃·Y^m₁))^m₂
```

We adapted this for vectors. Instead of luminance, we apply perceptual curves to embedding magnitudes. Dimensions with high variance get more precision. Dimensions near zero get less.

Result: 98.5% recall at the same 8 bytes per vector.

## Display-Aware Encoding

This is where it gets wild.

Different displays have different capabilities:
- SDR monitors: 100 nits peak
- HDR10: 1000 nits
- HDR4000: 4000 nits

We built a `DisplayAwareQuantizer` that adapts encoding based on target display characteristics. For vector databases, "display" means "downstream model" — what precision does the consumer actually need?

A lightweight retrieval model doesn't need the same precision as a fine-grained reranker.

## The Implementation

Our final quantization system:

```cpp
namespace vdb::quantization {
    class ProductQuantizer;       // Standard PQ
    class ScalarQuantizer;        // Simple 8-bit
    class DisplayAwareQuantizer;  // Perceptual curves
    class AdaptiveQuantizer;      // Learns from data
}
```

Each has its place:
- **ProductQuantizer**: Maximum compression, good recall
- **ScalarQuantizer**: Fast, simple, baseline
- **DisplayAwareQuantizer**: When perception matters
- **AdaptiveQuantizer**: When you have training data

## What We Learned

Compression isn't about throwing away data. It's about throwing away the *right* data — the parts that don't matter for your use case.

In images, that's imperceptible details.
In audio, that's inaudible frequencies.
In vectors, that's... well, we're still figuring that out.

But the journey was worth it. We're shipping a quantization system that's 10-15% better than standard approaches, with zero additional latency.

---

*Sometimes the rabbit hole is the destination.*</script>

    <script type="text/markdown" id="md-blog_posts-hybrid-search-journey"># Hybrid Search: Where Vectors Meet Words

*January 5, 2026*

---

A user searched for "Apple M3 chip performance benchmarks" and got results about fruit orchards.

This is the semantic search failure mode. The embedding model understood "Apple" as a general concept but missed that in this context, it's a company name. It understood "chip" but associated it with potato chips in some training examples.

Pure semantic search is amazing. Until it isn't.

## The Keyword Problem

Meanwhile, traditional search has the opposite problem. Search for "running shoes" and you won't find "athletic footwear" even though they're synonyms.

Keywords are precise but brittle.
Semantics are flexible but imprecise.

The answer, obviously, is both.

## Building BM25 From Scratch

BM25 (Best Match 25) is the algorithm behind Elasticsearch, Lucene, and most production search systems. The formula looks scary:

```
score(D, Q) = Σ IDF(qi) · (f(qi, D) · (k₁ + 1)) / (f(qi, D) + k₁ · (1 - b + b · |D|/avgdl))
```

But it's really just three intuitions:
1. **IDF**: Rare terms matter more ("the" appears everywhere, ignore it)
2. **TF saturation**: First occurrence matters most, diminishing returns after
3. **Length normalization**: Don't bias toward longer documents

We implemented BM25 in C++ with:
- Porter stemming
- Stop word removal  
- Inverted index with posting lists

Search over 1M documents: 8ms. Fast enough.

## The Fusion Problem

Now you have two lists of results:
1. Vector search: results ranked by cosine similarity
2. BM25 search: results ranked by relevance score

How do you combine them?

### Option 1: Weighted Sum
```
final_score = α · vector_score + (1-α) · bm25_score
```
Simple, but the scales are different. BM25 scores might be 0-20, while cosine similarity is 0-1.

### Option 2: Rank Fusion (RRF)
```
RRF_score = Σ 1 / (k + rank_i)
```
This ignores actual scores and just uses rank positions. Surprisingly effective.

### Option 3: Learned Fusion
Train a model to combine scores. More complex, potentially better, requires training data.

We implemented all three. **RRF won for simplicity**. With k=60, it handles most cases well. For power users, we expose the weighted sum with configurable α.

## When Hybrid Shines

Hybrid search crushes pure approaches when queries contain:

- **Named entities**: "NVIDIA 4090 vs AMD 7900 XTX"
- **Product codes**: "iPhone 15 Pro Max A17"
- **Technical terms**: "HNSW ef_construction parameter"
- **Mixed intent**: "best practices kubernetes secrets" (concept + keyword)

For pure conceptual queries ("what is the meaning of life"), vector search alone is fine. For pure keyword queries ("error code 0x8007007E"), BM25 alone is fine.

Hybrid handles everything in between.

## The API

```
results = db.hybrid_search(
    query="Apple M3 chip benchmarks",
    k=10,
    fusion="rrf",          # or "weighted"
    vector_weight=0.7,     # for weighted fusion
    lexical_weight=0.3
)
```

Clean and simple. The complexity is hidden where it should be.

## Lessons Learned

1. **Don't choose between paradigms** — combine them
2. **RRF is underrated** — rank-based fusion is robust and simple
3. **Let users control the blend** — different queries need different weights
4. **Keyword matching isn't dead** — it's a feature, not a bug

---

*Two weeks until v4.0 release. The finish line is in sight.*
</script>

    <script type="text/markdown" id="md-blog_posts-release-day"># Release Day: Hektor 4.0 is Live

*January 22, 2026*

---

We shipped.

After four months of architecture, implementation, debugging, and more debugging — Hektor 4.0 is live.

## What's in the Box

### Core Engine (C++23)
- **HNSW Index**: Sub-millisecond search at 99%+ recall
- **SIMD Optimized**: AVX-512 distance functions (8x speedup)
- **Memory-Mapped Storage**: Handles datasets larger than RAM

### Hybrid Search
- **BM25 Engine**: Full-text search with proper linguistics
- **RRF Fusion**: Rank-based score combination
- **Query Rewriting**: Synonym expansion, spell correction

### Quantization System
- **Product Quantizer**: 8-32x compression
- **Perceptual Curves**: PQ, HLG, adaptive gamma
- **Display-Aware Encoding**: Downstream-optimized precision

### Integrations
- **Python Bindings**: First-class pyvdb module
- **REST API**: FastAPI server with JWT auth
- **OpenTelemetry**: Distributed tracing built-in

### Extras
- **LLM Engine**: Local inference via llama.cpp
- **RAG Toolkit**: Chunking, context building, reranking
- **Multi-format Ingest**: CSV, JSON, PDF, Excel, Parquet, and more

## The Numbers

Benchmarked on SIFT-1M (1 million 128-dim vectors):

| Metric | Hektor 4.0 |
|--------|------------|
| Recall@10 | 99.2% |
| Query Latency (p99) | 2.8ms |
| Build Time | 45 seconds |
| Memory | 1.2 GB |

We're competitive with the best open-source options and beat several commercial offerings.

## What We Learned

**The hard parts weren't where we expected.**

Memory management? Straightforward.
SIMD optimization? Tedious but tractable.
HNSW implementation? Well-documented.

The hard parts:
- **Edge cases in hybrid search fusion** — what happens when BM25 returns nothing?
- **Unicode handling in tokenization** — three weeks of pain
- **Cross-platform builds** — CMake is powerful and frustrating

**Tests save you.** We have 200+ test cases. Every one caught a real bug.

**Docs are a feature.** We spent 20% of our time on documentation. It shows.

## What's Next

This is 4.0, not 1.0 that we're calling 4.0 for marketing. It's genuinely the fourth major architecture.

For 4.1:
- GPU acceleration for embedding generation
- Distributed mode (sharding + replication)
- Improved adaptive quantization

For 5.0 (someday):
- Neural index structures
- End-to-end differentiable search
- Things we haven't imagined yet

## Thank You

To everyone who gave feedback, reported bugs, asked hard questions — thank you. Building in public is only possible because people engage.

The code is MIT licensed. The research is open. Use it, break it, improve it.

---

*This is just the beginning.*

```
pip install pyvdb
```

*Let's build something great.*

— *The Hektor Team*</script>
    <script>
        // Configure marked for better rendering
        marked.setOptions({
            gfm: true,
            breaks: true,
            headerIds: true,
            mangle: false
        });

        // Helper to read preloaded markdown script tags
        function preloadMarkdown(file) {
            if (!file) return null;
            // try multiple id formats to match embedded script ids
            const base = file.replace(/[\/.]/g, '-');
            const candidates = [
                'md-' + base,                 // md-blog_posts-the-beginning-md
                'md-' + base.replace(/-md$/, '') // md-blog_posts-the-beginning
            ];
            for (const id of candidates) {
                const el = document.getElementById(id);
                if (el) return el.textContent;
            }
            return null;
        }

        // Document Data
        const documents = {
            blog: [
                        {
                            id: 'the-beginning',
                            title: 'The Beginning: Why We Started Building Hektor',
                            description: 'Every project starts with frustration. Ours was with existing vector databases — too slow, too complex, too many compromises.',
                            date: '2025-11-15',
                            category: 'Origins',
                            status: 'Published',
                            order: 1,
                            file: 'blog_posts/the-beginning.md'
                        },
                        {
                            id: 'choosing-hnsw',
                            title: 'Why HNSW Won: Choosing Our Index Algorithm',
                            description: 'We evaluated every major ANN algorithm. Here\'s why Hierarchical Navigable Small World graphs came out on top.',
                            date: '2025-12-03',
                            category: 'Architecture',
                            status: 'Published',
                            order: 2,
                            file: 'blog_posts/choosing-hnsw.md'
                        },
                        {
                            id: 'quantization-deep-dive',
                            title: 'The Quantization Rabbit Hole',
                            description: 'We thought compression was simple. Then we discovered perceptual curves, display-aware encoding, and the beautiful math of human vision.',
                            date: '2025-12-18',
                            category: 'Deep Dive',
                            status: 'Published',
                            order: 3,
                            file: 'blog_posts/quantization-rabbit-hole.md'
                        },
                        {
                            id: 'hybrid-search-journey',
                            title: 'Hybrid Search: Where Vectors Meet Words',
                            description: 'Pure semantic search misses keywords. Pure keyword search misses meaning. The solution requires both.',
                            date: '2026-01-05',
                            category: 'Architecture',
                            status: 'Published',
                            order: 4,
                            file: 'blog_posts/hybrid-search-journey.md'
                        },
                        {
                            id: 'release-day',
                            title: 'Release Day: Hektor 4.0 is Live',
                            description: 'After months of building, testing, and refining — we shipped. Here\'s what\'s in the box and what comes next.',
                            date: '2026-01-22',
                            category: 'Release',
                            status: 'Published',
                            order: 5,
                            file: 'blog_posts/release-day.md'
                        }
            ],
                articles: [
                    {
                        id: 'neural-latent-quantization',
                        title: 'Neural Latent Quantization: Learning Optimal Perceptual Bases',
                        description: 'Exploring semantic-space quantization where bits follow meaning rather than amplitude using learned latent representations.',
                        date: '2026-01-20',
                        category: 'Advanced Research',
                        status: 'Research',
                        order: 1,
                        content: `# Neural Latent Quantization: Learning Optimal Perceptual Bases

    ## Executive Summary

    This paper explores **Neural Latent Quantization (NLQ)**, a paradigm shift from traditional signal-space quantization to **semantic-space quantization** where bits follow meaning rather than amplitude. Unlike conventional quantizers that operate on raw signals or hand-crafted features, NLQ uses learned latent representations from autoencoders to discover optimal perceptual bases for compression.

    **Key Insight**: Quantize the **meaning** (latent manifold), not the **waveform** (signal space).

    ---

    ## Table of Contents

    1. [Introduction & Motivation](#introduction)
    2. [Theoretical Foundation](#theory)
    3. [Autoencoder Architecture](#architecture)
    4. [Latent Manifold Geometry](#manifold)
    5. [Quantization in Latent Space](#quantization)
    6. [Mathematical Formulation](#mathematics)
    7. [Implementation Strategy](#implementation)
    8. [Experimental Design](#experiments)
    9. [Performance Analysis](#performance)
    10. [Future Directions](#future)

    ---

    ## 1. Introduction & Motivation {#introduction}

    ### The Problem with Traditional Quantization

    Traditional quantization operates on **signal space**:
    ```
    Signal Space:
    x ∈ ℝⁿ → Q(x) → x̂ ∈ {q₁, q₂, ..., qₖ}

    Problem: Equal quantization steps ≠ equal perceptual steps
    ```

    **Example**: In an RGB image
    - Quantizing [255, 255, 255] vs [254, 254, 254]: Perceptually identical
    - Quantizing [128, 0, 0] vs [127, 0, 0]: Perceptually noticeable (red shift)

    ### Neural Latent Quantization Philosophy

    ```
    Signal Space → Encoder → Latent Space → Quantizer → Decoder → Reconstruction
            x                    z              ẑ                      x̂

    Key: Quantize z (semantic), not x (signal)
    ```

    **Advantages**:
    1. **Perceptual Alignment**: Latent dimensions encode perceptual features
    2. **Dimensionality Reduction**: n-dim signal → d-dim latent (d << n)
    3. **Learned Bases**: Data-driven rather than hand-crafted
    4. **Rate-Distortion Optimization**: Direct optimization for compression

    ---

    (document truncated for inline representation)
    `
                    },
                    {
                        id: 'temporal-redundancy',
                        title: 'Temporal Redundancy Beyond Motion Vectors',
                        description: 'Research on event-based encoding, delta-saliency, and memory-aware streaming for perceptual change encoding.',
                        date: '2026-01-20',
                        category: 'Advanced Research',
                        status: 'Research',
                        order: 2,
                        content: `# Temporal Redundancy Beyond Motion Vectors
    ## Quantizing Change in Perception, Not Change in Pixels

    ## Executive Summary

    Traditional video compression relies on **motion vectors** to predict pixel changes between frames. This paper proposes a radical departure: **quantize change in perception**, not change in pixels. We explore three novel approaches:

    1. **Event-Based Encoding**: Transmit only perceptually significant changes
    2. **Delta-Saliency Quantization**: Allocate bits based on what changed perceptually
    3. **Memory-Aware Streaming**: Model what the viewer already "knows" and transmit only what's new to perception

    **Core Insight**: Temporal redundancy exists in **perceptual space**, not just pixel space.

    ---

    (document truncated for inline representation)
    `
                    },
                    {
                        id: 'quantum-quantization',
                        title: 'Quantum Quantization: Superposition-Based Compression',
                        description: 'Theoretical framework for data compression using quantum computing principles including superposition and entanglement.',
                        date: '2026-01-20',
                        category: 'Theoretical',
                        status: 'Speculative',
                        order: 3,
                        content: `# Quantum Quantization
    ## Exploiting Quantum Superposition for Information Compression

    ## Executive Summary

    This paper explores **Quantum Quantization (QQ)**, a theoretical framework for data compression using quantum computing principles. Unlike classical quantization which maps continuous values to discrete bins, quantum quantization leverages:

    1. **Quantum Superposition**: Store multiple states simultaneously
    2. **Entanglement**: Correlate distant data points without classical communication
    3. **Quantum Interference**: Amplify relevant information, suppress noise

    **Status**: ⚠️ **Highly Speculative** - Requires quantum hardware not yet available at scale

    ---

    (document truncated for inline representation)
    `
                    }
                ],
            research: [
                {
                    id: 'vector-space-theory',
                    title: 'Vector Space Theory in High-Dimensional Embeddings',
                    description: 'Comprehensive mathematical treatment of vector spaces, distance metrics, and their implications for similarity search.',
                    date: '2026-01-04',
                    category: 'Foundations',
                    status: 'Peer-Reviewed',
                    order: 1,
                    content: `# Vector Space Theory in High-Dimensional Embeddings: A Comprehensive Study
> **Non-Euclidean Geometries, Distance Metrics, and Practical Implications for Similarity Search**

**Authors**: Artifact Virtual Research
**Last Updated**: January 4, 2026  
**Version**: 1.0  
**Status**: Peer-Reviewed Academic Research

---

## Abstract

This paper provides a comprehensive mathematical treatment of vector space theory as applied to high-dimensional embedding spaces in modern machine learning and information retrieval systems. We examine the fundamental properties of vector spaces, distance metrics, dimensionality considerations, and their practical implications for similarity search. Our analysis combines rigorous mathematical proofs with empirical observations from large-scale vector database implementations, with specific focus on the challenges and opportunities presented by high-dimensional spaces (d > 100).

---

(document truncated for inline representation)
`
                },
                {
                    id: 'hnsw-algorithm',
                    title: 'HNSW Graphs: Theory, Implementation, and Analysis',
                    description: 'Complete treatment of Hierarchical Navigable Small World graphs for approximate nearest neighbor search.',
                    date: '2026-01-04',
                    category: 'Algorithms',
                    status: 'Peer-Reviewed',
                    order: 2,
                    content: `# HNSW Graphs
> **Hierarchical Navigable Small World (HNSW) Graphs for Approximate Nearest Neighbor Search** <br> *Theory, Implementation, and Analysis*

**Authors**: Artifact Virtual Research  
**Last Updated**: January 4, 2026  
**Version**: 1.0  
**Status**: Peer-Reviewed Academic Research

---

## Abstract

This paper provides a comprehensive treatment of Hierarchical Navigable Small World (HNSW) graphs, a state-of-the-art data structure for approximate nearest neighbor search in high-dimensional spaces. We present rigorous theoretical analysis, algorithmic details, implementation considerations, and empirical performance characteristics. Our analysis demonstrates that HNSW achieves $O(\\log n)$ search complexity while maintaining high recall, making it the leading solution for large-scale similarity search applications.

---

(document truncated for inline representation)
`
                },
                {
                    id: 'perceptual-quantization',
                    title: 'Perceptual Quantization: Research & Implementation',
                    description: 'Theoretical foundations and practical applications of perceptual quantization for vector compression.',
                    date: '2026-01-04',
                    category: 'Quantization',
                    status: 'Peer-Reviewed',
                    order: 3,
                    content: `# Perceptual Quantization: Research & Implementation Analysis
> **Theoretical Foundations and Practical Applications**

**Authors**: Artifact Virtual Research  
**Last Updated**: January 4, 2026  
**Version**: 1.0  
**Status**: Peer-Reviewed Academic Research

---

## Executive Summary

This document explores the implications of implementing perceptual quantization (PQ/SQ curve-based quantization) as an enhancement to the existing vector quantization system in Hektor VectorDB. We analyze three advanced quantization paradigms and provide implementation recommendations.

---

(document truncated for inline representation)
`
                }
            ],
            publications: [
                {
                    id: 'research-survey',
                    title: 'Research Survey: State-of-the-Art in Vector Databases',
                    description: 'Comprehensive overview of 50+ papers covering vector databases, similarity search, and embeddings from 1954-2026.',
                    date: '2026-01-04',
                    category: 'Survey',
                    status: 'Published',
                    order: 1,
                    content: `# Research Survey: State-of-the-Art in Vector Databases and Similarity Search

**Last Updated**: January 4, 2026  
**Authors**: Artifact Virtual Research  
**Version**: 1.0

---

## Executive Summary

This survey provides a comprehensive overview of the current state of research and practice in vector databases, similarity search algorithms, and high-dimensional embeddings. We cover 50+ papers from 1954-2026, analyzing trends, breakthrough innovations, and future directions.

---

(document truncated for inline representation)
`
                },
                {
                    id: 'opentelemetry-tracing',
                    title: 'OpenTelemetry Distributed Tracing: 2026 Standards',
                    description: 'Comprehensive analysis of OpenTelemetry distributed tracing following AWS X-Ray\'s transition to OTel.',
                    date: '2026-01-08',
                    category: 'Systems',
                    status: 'Peer-Reviewed',
                    order: 2,
                    content: `# OpenTelemetry Distributed Tracing: A Comprehensive Study of 2026 Standards and Best Practices

**Research Paper**  
**Authors**: Vector Studio Research Team  
**Date**: January 8, 2026  
**Version**: 1.0  
**Status**: Peer-Reviewed Internal Research

---

## Abstract

This paper presents a comprehensive analysis of OpenTelemetry distributed tracing as of January 2026, following the major industry shift marked by AWS X-Ray's transition to OpenTelemetry as the primary instrumentation framework.

---

(document truncated for inline representation)
`
                }
            ]
        };

        // Render blog cards (different style)
        function renderBlogCards(containerId) {
            const container = document.getElementById(containerId);
            const items = documents.blog.sort((a, b) => new Date(b.date) - new Date(a.date)); // Newest first
            
            container.innerHTML = items.map(doc => `
                <div class="blog-card" data-id="${doc.id}" data-section="blog">
                    <div class="card-date">${formatDate(doc.date)}</div>
                    <h3>${doc.title}</h3>
                    <p class="card-description">${doc.description}</p>
                    <div class="read-more">
                        Read more
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7"/>
                        </svg>
                    </div>
                </div>
            `).join('');

            container.querySelectorAll('.blog-card').forEach(card => {
                card.addEventListener('click', () => openModal('blog', card.dataset.id));
            });
        }

        // Render document cards
        function renderDocCards(section, containerId) {
            const container = document.getElementById(containerId);
            const items = documents[section].sort((a, b) => a.order - b.order);
            
            if (items.length === 0) {
                container.innerHTML = `
                    <div style="text-align: center; padding: 60px; color: var(--text-muted);">
                        <p>No content available yet.</p>
                    </div>
                `;
                return;
            }

            container.innerHTML = items.map(doc => {
                const statusClass = doc.status.toLowerCase().includes('speculative') ? 'speculative' : 
                                   doc.status.toLowerCase().includes('draft') || doc.status.toLowerCase().includes('research') ? 'draft' : '';
                return `
                    <div class="doc-card" data-id="${doc.id}" data-section="${section}">
                        <div class="card-meta">
                            <span class="card-category">${doc.category}</span>
                            <span class="card-order">#${String(doc.order).padStart(2, '0')}</span>
                        </div>
                        <h3>${doc.title}</h3>
                        <p class="card-description">${doc.description}</p>
                        <div class="card-footer">
                            <span class="card-date">${formatDate(doc.date)}</span>
                            <span class="card-status ${statusClass}">${doc.status}</span>
                        </div>
                    </div>
                `;
            }).join('');

            container.querySelectorAll('.doc-card').forEach(card => {
                card.addEventListener('click', () => openModal(card.dataset.section, card.dataset.id));
            });
        }

        function formatDate(dateStr) {
            const date = new Date(dateStr);
            return date.toLocaleDateString('en-US', { month: 'long', day: 'numeric', year: 'numeric' });
        }

        // Modal handling
        async function openModal(section, id) {
            const doc = documents[section].find(d => d.id === id);
            if (!doc) return;

            document.getElementById('modal-title').textContent = doc.title;
            document.getElementById('modal-meta').textContent = `${doc.category} · ${formatDate(doc.date)}`;
            
            // Show loading state
            document.getElementById('modal-content').innerHTML = '<div class="loading-spinner">Loading article...</div>';
            document.getElementById('modal-overlay').classList.add('active');
            document.body.style.overflow = 'hidden';
            
            let content = doc.content || (doc.file ? preloadMarkdown(doc.file) : null);

                // If no inline content or preloaded script, try to fetch from file
                if (!content && doc.file) {
                    try {
                        const response = await fetch(doc.file);
                        if (response.ok) {
                            content = await response.text();
                            // Remove YAML frontmatter
                            content = content.replace(/^---[\s\S]*?---\s*/m, '');
                        }
                    } catch (e) {
                        console.error('Fetch error:', e);
                        content = `# ${doc.title}

> This document is available at ${doc.file}

---

**Note**: Some browsers block loading local files when the page is opened via the file:// protocol. For full rendering, view this page over HTTP (for example, GitHub Pages).`;
                    }
                }
            
            document.getElementById('modal-content').innerHTML = marked.parse(content || '*No content available.*');
        }

        function closeModal() {
            document.getElementById('modal-overlay').classList.remove('active');
            document.body.style.overflow = '';
        }

        // Section navigation
        document.querySelectorAll('.section-nav button').forEach(btn => {
            btn.addEventListener('click', () => {
                document.querySelectorAll('.section-nav button').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');
                
                document.querySelectorAll('.content-section').forEach(s => s.classList.remove('active'));
                document.getElementById(`${btn.dataset.section}-section`).classList.add('active');
            });
        });

        // Modal close handlers
        document.getElementById('modal-close').addEventListener('click', closeModal);
        document.getElementById('modal-overlay').addEventListener('click', (e) => {
            if (e.target === e.currentTarget) closeModal();
        });
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape') closeModal();
        });

        // Mobile menu
        const mobileMenuBtn = document.getElementById('mobile-menu-btn');
        const mobileSidebar = document.getElementById('mobile-sidebar');
        
        mobileMenuBtn.addEventListener('click', () => {
            mobileMenuBtn.classList.toggle('active');
            mobileSidebar.classList.toggle('active');
        });

        mobileSidebar.querySelectorAll('a').forEach(link => {
            link.addEventListener('click', () => {
                mobileMenuBtn.classList.remove('active');
                mobileSidebar.classList.remove('active');
            });
        });

        // Initialize
        renderBlogCards('blog-grid');
        renderDocCards('articles', 'articles-grid');
        renderDocCards('research', 'research-grid');
        renderDocCards('publications', 'publications-grid');
    </script>
</body>
</html>
