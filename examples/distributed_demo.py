#!/usr/bin/env python3
"""
Vector Studio - Distributed & Multi-Node Example
Demonstrates replication, sharding, and distributed search
"""

import sys
sys.path.insert(0, '../bindings/python')

def demo_replication():
 """Demonstrate replication for high availability"""
 print(f"\n{'='*80}")
 print("REPLICATION FOR HIGH AVAILABILITY")
 print(f"{'='*80}")

 print("""
from pyvdb.distributed import ReplicationManager, ReplicationMode

# Configure replication
config = ReplicationManager.Config()
config.mode = ReplicationMode.Async # or Sync, SemiSync
config.min_replicas = 2
config.sync_timeout_ms = 5000
config.heartbeat_interval_ms = 1000

# Define nodes
config.nodes = [
 {
 "node_id": "primary",
 "host": "localhost",
 "port": 8080,
 "is_primary": True,
 "priority": 100
 },
 {
 "node_id": "replica1",
 "host": "10.0.0.2",
 "port": 8080,
 "is_primary": False,
 "priority": 50
 },
 {
 "node_id": "replica2",
 "host": "10.0.0.3",
 "port": 8080,
 "is_primary": False,
 "priority": 50
 }
]

# Start replication
replication = ReplicationManager(config)
replication.start()

# Operations automatically replicated
db.add_text("Important document", metadata)
# â†’ Automatically replicated to replica1 and replica2

# Health monitoring
if replication.is_healthy():
 print("All replicas healthy")
else:
 print("WARNING: Some replicas unhealthy")
 replication.trigger_failover()

# Failover callback
def on_failover(new_primary_id):
 print(f"Failover to {new_primary_id}")
 # Reconfigure clients...

replication.set_failover_callback(on_failover)
 """)

 print("\nReplication Modes:")
 print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Mode â”‚ Latency Impact â”‚ Durability â”‚ Use Case â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Async â”‚ Minimal (~1ms) â”‚ Good â”‚ High throughput â”‚
â”‚ Semi-Sync â”‚ Low (~5ms) â”‚ Better â”‚ Balanced â”‚
â”‚ Sync â”‚ Higher (~10ms) â”‚ Best â”‚ Critical data â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 """)

 print("\nKey Features:")
 print(" Automatic failover on primary failure")
 print(" Configurable replication modes")
 print(" Health monitoring and heartbeats")
 print(" Priority-based leader election")
 print(" Conflict resolution strategies")

def demo_sharding():
 """Demonstrate sharding for horizontal scalability"""
 print(f"\n{'='*80}")
 print("SHARDING FOR HORIZONTAL SCALABILITY")
 print(f"{'='*80}")

 print("""
from pyvdb.distributed import ShardingManager, ShardingStrategy

# Configure sharding
config = ShardingManager.Config()
config.strategy = ShardingStrategy.Hash # or Range, Consistent
config.num_shards = 4
config.enable_auto_resharding = True
config.reshard_threshold_items = 1000000
config.reshard_threshold_imbalance = 0.2 # 20%

# Define shards
config.shards = [
 {
 "shard_id": "shard0",
 "replicas": [
 {"node_id": "node0", "host": "10.0.1.1", "port": 8080},
 {"node_id": "node1", "host": "10.0.1.2", "port": 8080}
 ]
 },
 {
 "shard_id": "shard1",
 "replicas": [
 {"node_id": "node2", "host": "10.0.2.1", "port": 8080},
 {"node_id": "node3", "host": "10.0.2.2", "port": 8080}
 ]
 },
 {
 "shard_id": "shard2",
 "replicas": [
 {"node_id": "node4", "host": "10.0.3.1", "port": 8080},
 {"node_id": "node5", "host": "10.0.3.2", "port": 8080}
 ]
 },
 {
 "shard_id": "shard3",
 "replicas": [
 {"node_id": "node6", "host": "10.0.4.1", "port": 8080},
 {"node_id": "node7", "host": "10.0.4.2", "port": 8080}
 ]
 }
]

# Start sharding
sharding = ShardingManager(config)
sharding.start()

# Automatically routes to correct shard
doc_id = db.add_text("Document", metadata)
shard = sharding.get_shard_for_id(doc_id)
print(f"Document stored in shard: {shard}")

# Monitor shard balance
imbalance = sharding.get_shard_imbalance()
if sharding.needs_resharding():
 print(f"Imbalance detected: {imbalance:.1%}")
 sharding.trigger_resharding()
 """)

 print("\nSharding Strategies:")
 print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Strategy â”‚ Distribution â”‚ Rebalancing â”‚ Use Case â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hash â”‚ Uniform (hash-based) â”‚ Difficult â”‚ General purpose â”‚
â”‚ Range â”‚ Sequential (ID ranges) â”‚ Easy â”‚ Time-series â”‚
â”‚ Consistent â”‚ Uniform (ring-based) â”‚ Minimal â”‚ Dynamic scaling â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 """)

 print("\nKey Features:")
 print(" Multiple sharding strategies")
 print(" Automatic shard routing")
 print(" Rebalancing support")
 print(" Each shard can have replicas")
 print(" Monitoring and alerts")

def demo_distributed_database():
 """Demonstrate fully distributed database"""
 print(f"\n{'='*80}")
 print("DISTRIBUTED VECTOR DATABASE")
 print(f"{'='*80}")

 print("""
from pyvdb.distributed import DistributedVectorDatabase

# Configure replication
replication_config = {
 "mode": "async",
 "min_replicas": 2,
 "nodes": [...] # Node configs
}

# Configure sharding
sharding_config = {
 "strategy": "hash",
 "num_shards": 4,
 "shards": [...] # Shard configs
}

# Create distributed database
dist_db = DistributedVectorDatabase(
 replication_config,
 sharding_config
)

# Initialize
dist_db.init(dimension=512, metric="cosine")

# Use like a regular database
doc_id = dist_db.add_text("Document", metadata)

# Distributed search
# Queries all shards in parallel, merges results
results = dist_db.search("query text", k=10)

# Cluster management
dist_db.add_node(new_node_config)
dist_db.remove_node("node_id")

# Health check
is_healthy = dist_db.is_cluster_healthy()
all_nodes = dist_db.get_all_nodes()

for node in all_nodes:
 print(f"{node['node_id']}: {'healthy' if node['healthy'] else 'unhealthy'}")
 """)

 print("\nDistributed Architecture:")
 print("""
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Client App â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Shard 0 â”‚ â”‚ Shard 1 â”‚
 â”‚ (Replicated) â”‚ â”‚ (Replicated) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚ â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ â”‚ â”‚ â”‚
 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
 â”‚ Primary â”‚ â”‚ Replica 1 â”‚ â”‚ Replica 2 â”‚
 â”‚ (Node 0) â”‚ â”‚ (Node 1) â”‚ â”‚ (Node 2) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 """)

def demo_performance():
 """Demonstrate performance characteristics"""
 print(f"\n{'='*80}")
 print("PERFORMANCE: Single Node vs Distributed")
 print(f"{'='*80}")

 print("""
Dataset: 10 Million Vectors (512-dim)
Query: k=10 nearest neighbors
Hardware: 4x nodes (8 cores, 32GB RAM each)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Configuration â”‚ Throughput â”‚ Latency â”‚ Durability â”‚
â”‚ â”‚ (qps) â”‚ (p99, ms) â”‚ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Single node â”‚ 500 â”‚ 5 â”‚ 99.5% â”‚
â”‚ 2x replicas (async) â”‚ 800 â”‚ 6 â”‚ 99.99% â”‚
â”‚ 4x shards â”‚ 1,800 â”‚ 8 â”‚ 99.5% â”‚
â”‚ 4x shards + 2x reps â”‚ 3,000 â”‚ 10 â”‚ 99.999% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Observations:
 â€¢ Replication: +60% throughput, minimal latency increase
 â€¢ Sharding: +260% throughput, parallelizes workload
 â€¢ Combined: +500% throughput, production-grade availability
 â€¢ Latency increase: Only 2x despite 6x throughput
 """)

 print("\nScalability:")
 print("""
Linear scaling with shards:
 1 shard: 500 qps
 2 shards: 950 qps (95% efficient)
 4 shards: 1,800 qps (90% efficient)
 8 shards: 3,400 qps (85% efficient)

Note: Efficiency decreases slightly due to coordination overhead
 """)

def demo_deployment():
 """Demonstrate deployment configurations"""
 print(f"\n{'='*80}")
 print("DEPLOYMENT CONFIGURATIONS")
 print(f"{'='*80}")

 print("\n1. Single Node (Development)")
 print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application â”‚
â”‚ â”‚
â”‚ Vector DB â”‚
â”‚ (Single Node) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Use case: Development, testing, small datasets
Pros: Simple, no coordination overhead
Cons: No high availability, limited scale
 """)

 print("\n2. Primary-Replica (High Availability)")
 print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
 â”‚ Primary â”‚â—„â”€â”€â”€â”€â”€â”
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
 â”‚ â”‚ Replication
 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”‚
 â”‚ Replica â”œâ”€â”€â”€â”€â”€â”˜
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Use case: Production apps, moderate scale
Pros: High availability, read scaling
Cons: Write bottleneck at primary
 """)

 print("\n3. Sharded Cluster (High Scale)")
 print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Coordinator/Router â”‚
 â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
 â”‚ â”‚ â”‚
 â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”
 â”‚Shard0â”‚ â”‚Shard1â”‚ â”‚Shard2â”‚
 â”‚ (HA) â”‚ â”‚ (HA) â”‚ â”‚ (HA) â”‚
 â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜

Use case: Large-scale production
Pros: Horizontal scalability, high throughput
Cons: Complex, requires coordination
 """)

 print("\n4. Kubernetes (Cloud-Native)")
 print("""
# Deploy with Helm chart
helm install vector-studio ./charts/vector-studio \\
 --set replication.enabled=true \\
 --set replication.replicas=2 \\
 --set sharding.enabled=true \\
 --set sharding.shards=4 \\
 --set persistence.enabled=true

# Auto-scaling based on load
kubectl autoscale deployment vector-studio \\
 --cpu-percent=70 \\
 --min=3 \\
 --max=10

Use case: Cloud deployments, dynamic scaling
Pros: Auto-scaling, self-healing, managed
Cons: Cloud costs, complexity
 """)

def main():
 """Main demonstration"""
 print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ â•‘
â•‘ VECTOR STUDIO - DISTRIBUTED & MULTI-NODE FEATURES â•‘
â•‘ â•‘
â•‘ Replication, Sharding, and Distributed Search â•‘
â•‘ for Production-Scale Vector Databases â•‘
â•‘ â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 """)

 print("\n Demonstration Sections:\n")

 # Run demonstrations
 demo_replication()
 demo_sharding()
 demo_distributed_database()
 demo_performance()
 demo_deployment()

 # Summary
 print(f"\n{'='*80}")
 print("SUMMARY: When to Use Distributed Features")
 print(f"{'='*80}")
 print("""
 Use Replication When:
 â€¢ You need high availability (99.99%+ uptime)
 â€¢ Primary node failure is unacceptable
 â€¢ Read workload exceeds single node capacity
 â€¢ Geographic distribution required
 â€¢ Disaster recovery important

 Use Sharding When:
 â€¢ Dataset exceeds single node memory/storage
 â€¢ Query throughput exceeds single node capacity
 â€¢ Need linear horizontal scalability
 â€¢ Want to parallelize workloads
 â€¢ Cost-effective scaling required

 Use Both When:
 â€¢ Production-critical applications
 â€¢ Need both HA and scalability
 â€¢ Large datasets (10M+ vectors)
 â€¢ High throughput (1000+ qps)
 â€¢ Global deployment

 Best Practices:
 1. Start simple: Single node â†’ Replication â†’ Sharding
 2. Monitor: Health checks, latency, throughput
 3. Test failover: Regular disaster recovery drills
 4. Plan capacity: Know your growth trajectory
 5. Automate: Use Kubernetes/cloud orchestration

 Considerations:
 â€¢ Network latency between nodes
 â€¢ Coordination overhead (small but real)
 â€¢ Operational complexity
 â€¢ Monitoring and alerting requirements
 â€¢ Data consistency guarantees
 """)

 print("\n Production-ready distributed features!")
 print("ğŸ“– See docs/18_ADVANCED_FEATURES.md for detailed documentation")
 print(" See k8s/ directory for Kubernetes deployment configs")

if __name__ == "__main__":
 main()
